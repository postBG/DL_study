{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Vs GRU\n",
    "\n",
    "\"These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio\n",
    "\n",
    "\"The GRU outperformed the LSTM on all tasks with the exception of language modelling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever\n",
    "\n",
    "\"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei\n",
    "\n",
    "\"Which of these variants is best? Do the differences matter? [Greff, et al. (2015)](https://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that theyâ€™re all about the same. [Jozefowicz, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah\n",
    "\n",
    "\"In our [Neural Machine Translation] experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells. Somewhat to our surprise, we found that the vanilla decoder is unable to learn nearly as well as the gated variant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906v2) by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RNN Architectures\n",
    "\n",
    "| Application                           | Cell | Layers  | Size          | Vocabulary                | Embedding Size | Learning Rate |       |\n",
    "|---------------------------------------|------|---------|---------------|---------------------------|----------------|---------------|-------|\n",
    "| Speech Recognition (large vocabulary) | LSTM | 5, 7    | 600, 1000     | 82K, 500K                 | --             | --            | [paper](https://arxiv.org/abs/1610.09975) |\n",
    "| Speech Recognition                    | LSTM | 1, 3, 5 | 250           | --                        | --             | 0.001         | [paper](https://arxiv.org/abs/1303.5778) |\n",
    "| Machine Translation (seq2seq)         | LSTM | 4       | 1000          | Source: 160K, Target: 80K | 1,000          | --            | [paper](https://arxiv.org/abs/1409.3215) |\n",
    "| Image Captioning                      | LSTM | --      | 512           | --                        | 512            | (fixed)       | [paper](https://arxiv.org/abs/1411.4555) |\n",
    "| Image Generation                      | LSTM | --      | 256, 400, 800 | --                        | --             | --            | [paper](https://arxiv.org/abs/1502.04623) |\n",
    "| Question Answering                    | LSTM | 2       | 500           | --                        | 300            | --            | [pdf](http://www.aclweb.org/anthology/P15-2116)   |\n",
    "| Text Summarization                    | GRU  |         | 200           | Source: 119K, Target: 68K | 100            | 0.001         | [pdf](https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf)   |"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
